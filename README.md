# Elemos Yar (Ø¹Ù„Ù…ÙˆØµ ÛŒØ§Ø±) - Intelligent Professor Analysis System

![Elemos Yar Overview](images/image.png)

## ğŸŒŸ Project Overview
**Elemos Yar** is an advanced data analytics platform designed to empower university students with data-driven insights for course selection. By analyzing unstructured student reviews from Telegram channel, the system builds comprehensive professor profiles, predicts approval ratings, and provides personalized recommendations.

The core innovation lies in its ability to process informal Persian text (Finglish, slang, typos) and extract structured metrics about **Teaching Quality**, **Grading Fairness**, and **Attendance Policies** without requiring manual labeling.

---

## ğŸ“‚ Project Architecture
The codebase is structured for scalability and modularity:

```text
d:\personal\Projects\Ø¹Ù„Ù…ÙˆØµ ÛŒØ§Ø±\
â”œâ”€â”€ app.py                      # ğŸš€ Main Streamlit Web Application
â”œâ”€â”€ recommender_system.py       # ğŸ§  Core Recommendation Engine Class
â”œâ”€â”€ README.md                   # ğŸ“„ Project Documentation (You are here)
â”œâ”€â”€ data/                       # ğŸ’¾ Data Storage Layer
â”‚   â”œâ”€â”€ raw/                    # Raw Input Data
â”‚   â”‚   â””â”€â”€ result.json         # Raw Telegram Export
â”‚   â”œâ”€â”€ processed/              # Intermediate Processed Data
â”‚   â”‚   â”œâ”€â”€ processed_comments.json # Parsed & Cleaned JSON
â”‚   â”‚   â””â”€â”€ normalized_comments.json # Enriched Dataset with Features & Sentiment
â”‚   â””â”€â”€ output/                 # Final Outputs
â”‚       â”œâ”€â”€ professor_approval_ratings.csv # Generated Leaderboard
â”‚       â””â”€â”€ professor_clusters.json # ML Cluster Assignments
â”œâ”€â”€ scripts/                    # ğŸ› ï¸ ETL & Analysis Pipeline
â”‚   â”œâ”€â”€ process_data.py         # ETL: Raw JSON Parser
â”‚   â”œâ”€â”€ normalize_data.py       # NLP: Persian Text Normalization
â”‚   â”œâ”€â”€ sentiment_analysis.py   # AI: BERT-based Sentiment Scoring
â”‚   â”œâ”€â”€ extract_features.py     # NLP: Rule-based Feature Extraction
â”‚   â”œâ”€â”€ binary_classifier.py    # ML: Approval Prediction Model
â”‚   â”œâ”€â”€ calculate_approval.py   # Stats: Approval Rating Aggregation
â”‚   â””â”€â”€ professor_clustering.py # ML: Unsupervised Clustering (K-Means)
â”œâ”€â”€ models/                     # ğŸ¤– Trained Model Artifacts
â”‚   â””â”€â”€ binary_classifier_model.pkl
â””â”€â”€ images/                     # ğŸ“Š Generated Static Plots
    â””â”€â”€ professor_clusters_plot.png
```

---

## ğŸ§  Algorithms & Models Explained

### 1. ğŸ“ Natural Language Processing (NLP) Pipeline
Before any analysis, we clean the raw Persian text to ensure consistency:
*   **Normalization**: We use `unicodedata` and custom regex to standardize Arabic/Persian characters (e.g., `ÙŠ` -> `ÛŒ`, `Ùƒ` -> `Ú©`).
*   **Digit Unification**: All numbers (Persian `Û±Û²Û³` or Arabic `Ù¡Ù¢Ù£`) are converted to English `123`.
*   **Entity Resolution**: Professor names are stripped of titles (Dr., Eng., Ostad) to merge duplicates (e.g., "Dr. Akbari" == "Akbari").

### 2. ğŸ­ Sentiment Analysis (Transfer Learning)
Instead of training a sentiment model from scratch, we employ **Transfer Learning**:
*   **Model Architecture**: BERT (Bidirectional Encoder Representations from Transformers).
*   **Pre-trained Weights**: `HooshvareLab/bert-fa-base-uncased-sentiment-digikala`.
*   **Logic**: This model was pre-trained on millions of Persian product reviews (Digikala). We "transfer" its understanding of positive/negative sentiment to the domain of university professor reviews.
*   **Output**: A continuous score (-1.0 to +1.0) representing the emotional tone of each comment.

### 3. ğŸ·ï¸ Automated Feature Extraction
We map unstructured text to structured categorical features using keyword density analysis:
*   **Grading**: Maps phrases like "Ø¯Ø³Øª Ø¨Ø§Ø²" (open hand) to `Lenient` and "Ù†Ù…Ø±Ù‡ Ù†Ù…ÛŒØ¯Ù‡" (doesn't give grades) to `Strict`.
*   **Attendance**: Maps "Ø­Ø¶ÙˆØ± Ù…Ù‡Ù… Ù†ÛŒØ³Øª" (attendance not important) to `Bonus/Optional`.
*   **Resources**: Detects mentions of "Ø¬Ø²ÙˆÙ‡" (notes), "Ø§Ø³Ù„Ø§ÛŒØ¯" (slides), or "Ú©ØªØ§Ø¨" (textbook).

### 4. ğŸ”® Binary Classification (Approval Predictor)
We built a model to predict if a *new, unrated* comment implies a "Good" or "Bad" experience.
*   **The Problem**: We had no explicit "Recommended" label in the raw text.
*   **The Solution (Proxy Labels)**: We generated training labels programmatically:
    *   `Label = 1 (Good)` if Student Evaluation Score â‰¥ **7.0/10**.
    *   `Label = 0 (Not Good)` if Score < **7.0**.
*   **Algorithm**: **Logistic Regression** with **TF-IDF Vectorization** (Unigrams + Bigrams).
*   **Performance**: The model identifies key positive words (e.g., "Ø¹Ø§Ù„ÛŒ", "Ø®ÙˆØ´ Ø¨Ø±Ø®ÙˆØ±Ø¯") and negative words (e.g., "Ø¨Ø¯ Ù†Ù…Ø±Ù‡", "Ù¾Ø§Ø³ Ù†Ù…ÛŒØ´ÛŒØ¯").

### 5. ğŸ¯ Recommendation Algorithm (Hybrid Weighted Scoring)
The `ProfessorRecommender` class uses a hybrid approach to rank professors for a specific course:
1.  **Filtering**: Narrows down professors who teach the requested course (substring match).
2.  **Preference Matching**: Calculates a `Match Score` based on user constraints (e.g., "I want Lenient Grading").
    *   It uses the *probability distribution* of traits. If a professor has 80% 'Lenient' reviews, they get a higher match score than someone with 20%.
3.  **Quality Scoring**: Combines `Average Sentiment` and `Average Evaluation Score`.
4.  **Final Ranking**: `Score = (Quality_Score * 0.4) + (Match_Score * 0.6)`.

### 6. ğŸ§© Unsupervised Clustering
We group professors into personas using **K-Means Clustering** (k=4):
*   **Features**: Average Score, Sentiment, Grading Strictness, Attendance Strictness.
*   **Dimensionality Reduction**: We use **PCA (Principal Component Analysis)** to project these multi-dimensional features into 2D for visualization.
*   **Outcome**: Identifies clusters like "Easy A & High Rated", "Strict & Low Rated", etc.

---

## ğŸ›¡ï¸ Privacy & Anonymization
To ensure the privacy of faculty members while maintaining the educational value of this dataset, all personal identifiers have been **anonymized**.
*   **Professor Names**: Replaced with unique IDs (e.g., `Ø§Ø³ØªØ§Ø¯ 101`, `Ø§Ø³ØªØ§Ø¯ 402`).
*   **Context Preservation**: The structural integrity of reviews (courses taught, sentiment, scores) is fully preserved to allow for reproducible research and analysis.
*   **Reproducibility**: The `anonymize_dataset.py` script is included to demonstrate the anonymization logic.

## ğŸš€ How to Run

### Step 1: Install Dependencies
```bash
pip install pandas numpy scikit-learn matplotlib seaborn plotly streamlit joblib torch transformers
```

### Step 2: Execute the Data Pipeline
Run the scripts in this specific order to process the data from scratch:

1.  **Parse Data**: `python scripts/process_data.py`
2.  **Normalize**: `python scripts/normalize_data.py`
3.  **Analyze Sentiment**: `python scripts/sentiment_analysis.py` (Note: Downloads ~500MB model)
4.  **Extract Features**: `python scripts/extract_features.py`
5.  **Train Models**: `python scripts/binary_classifier.py`
6.  **Calculate Stats**: `python scripts/calculate_approval.py`
7.  **Cluster**: `python scripts/professor_clustering.py`

### Step 3: Launch the Dashboard
Start the web interface:
```bash
streamlit run app.py
```
*   Access the app at `http://localhost:8501`.

---

## ğŸ“Š Dashboard Pages
1.  **Overview**: Global metrics, sentiment distribution, and the "Top 10" leaderboard.
2.  **Search**: Deep dive into individual professors. See their word clouds, trait distributions, and read raw comments.
3.  **Recommender**: Input a course name (e.g., "Riazi 1") and your preferences to get a ranked list of best-fit professors.

---

## âš ï¸ Common Issues & Troubleshooting
*   **"File not found"**: Ensure you run all scripts from the **root directory** (`d:\personal\Projects\Ø¹Ù„Ù…ÙˆØµ ÛŒØ§Ø±`), NOT from inside `scripts/`.
*   **"Model not found"**: You must run `binary_classifier_model.py` *before* running `calculate_approval.py` or the Streamlit app.
*   **Encoding Errors**: The scripts are set to use `utf-8`. If you see strange characters in Windows CMD, try setting `chcp 65001`.
